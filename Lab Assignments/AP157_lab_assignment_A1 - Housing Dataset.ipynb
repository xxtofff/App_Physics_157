{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Housing Dataset\n","\n","Suppose we want to buy a house from a neighbourhood, and we have data that contains the general characteristic of the neighborhood, houses, and the population itself. To temper our expectations, we want to predict the median house value."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd \n","impot numpy as np \n","\n","file_path = 'Datasets\\\\'\n","housing = pd.read_csv(file_path + 'housing.csv')\n","housing.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["housing.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["housing.describe()"]},{"cell_type":"markdown","metadata":{},"source":["We want to predict the `median_house_value` column. What we want to do is to separate the column we want to predict, or the target column, from the possible determinants that we will use for the prediction, or the feature columns. Then, we split the data into the training set and the test set."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","target_cols = ['median_house_value']\n","feature_cols = [col for col in housing.columns if col not in target_cols]\n","\n","x_full = housing[feature_cols]\n","y = housing[target_cols]\n","\n","x_train, x_test, y_train, y_test = train_test_split(x_full, y, train_size = 0.8, random_state = 0)"]},{"cell_type":"markdown","metadata":{},"source":["It is important to check if there are blank cells and the feature column where it is included so we can deal with it in the future. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["null_cols = [col for col in x_full.columns if x_full[col].isnull().any()]\n","null_cols"]},{"cell_type":"markdown","metadata":{},"source":["Check the amount of rows where there are no entries."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nan_count = x_full[null_cols].isnull().sum().sum()\n","print('There are {} rows with NaN values'.format(nan_count))"]},{"cell_type":"markdown","metadata":{},"source":["We will list the numerical and categorical columns."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_cols = [col for col in feature_cols if x_full[col].dtype in ['int64', 'float64']]\n","categorical_cols = [col for col in feature_cols if x_full[col].dtype in ['object']]\n","\n","print('The numerical columns are: {}'.format(num_cols))\n","print('The categorical columns are: {}'.format(categorical_cols))"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, the column with a null cell is numerical. We can preprocess the data by filling the null cell with the mean value. It is better than simply putting in 0 total bedrooms for a community."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","num_transformer = SimpleImputer(strategy = 'median')\n","cat_transformer = OneHotEncoder(handle_unknown = 'ignore')\n","preprocess = ColumnTransformer(transformers = [('num', num_transformer, num_cols), ('cat', cat_transformer, categorical_cols)])"]},{"cell_type":"markdown","metadata":{},"source":["Next, we will use a Random Forest Regressor with a max depth of 30."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error\n","import random\n","\n","depth = {}\n","for max_depth_val in random.choices([*range(1, 50, 1)], k = 10):\n","    model = RandomForestRegressor(max_depth = max_depth_val, random_state = 0)\n","    pipeline = Pipeline(steps = [('preprocessor', preprocess), ('model', model)])\n","    pipeline.fit(x_train, y_train.values.ravel())\n","    predicted_val = pipeline.predict(x_test)\n","    error = mean_squared_error(y_test, predicted_val)\n","    depth[max_depth_val] = error\n","optim_depth = min(depth, key = depth.get)\n","error = depth[optim_depth]\n","print('The root-mean-square error for a maximum depth of {} is {}.'.format(max_depth_val, np.sqrt(error)))"]},{"cell_type":"markdown","metadata":{},"source":["For comparison, we can check the actual and predicted values side-by-side."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = RandomForestRegressor(max_depth = optim_depth, random_state = 0)\n","pipeline = Pipeline(steps = [('preprocessor', preprocess), ('model', model)])\n","pipeline.fit(x_train, y_train.values.ravel())\n","predicted_val = pipeline.predict(x_test)\n","predicted_cols = pd.DataFrame(predicted_val, columns = ['predicted'], index = y_test.index)\n","comparison_table = y_test.join(predicted_cols)\n","comparison_table.head()"]},{"cell_type":"markdown","metadata":{},"source":["We can actually search for parameters that will make this error lower. For example, we can make loops that calculate the error for a corresponding parameter and find the parameter value that minimizes this error. If we will do this for all parameters, it may be computationally expensive. I want to try hyperparameter tuning as described in [this article](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74) but as we can see, the calculation for a maximum depth of 30 takes $>10$ secs. This may consume a lot of time so we will stop here for now."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
